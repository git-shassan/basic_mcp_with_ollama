# MCPHost Configuration File
# All command-line flags can be configured here
# This demonstrates the simplified local/remote/builtin server configuration
# MCP Servers configuration
# Add your MCP servers here
# Examples for different server types:

provider-url: "http://192.168.22.4:11434"
model: "ollama:llama3.2:3b"

mcpServers:
#   # Local MCP servers - run commands locally via stdio transport
#   filesystem-local:
#     type: "local"
#     command: ["npx", "@modelcontextprotocol/server-filesystem", "/"]
#     environment:
#       DEBUG: "true"
#       LOG_LEVEL: "info"
#   
#   sqlite:
#     type: "local" 
#     command: ["uvx", "mcp-server-sqlite", "--db-path", "/tmp/example.db"]
#     environment:
#       SQLITE_DEBUG: "1"
#   
#   # Builtin MCP servers - run in-process for optimal performance
   ddg-search:
     type: "local"
     command: "python /root/ddg.py"
     name: "ddg-search"

   filesystem-builtin:
     type: "builtin"
     name: "fs"
     options:
       allowed_directories: ["/root"]
       allowedTools: ["read_file", "write_file", "list_directory"]
#   
#   # Minimal builtin server - defaults to current working directory
#   filesystem-cwd:
#     type: "builtin"
#     name: "fs"
#   
#   # Bash server for shell commands
#   bash:
#     type: "builtin"
#     name: "bash"
#   
#   # Todo server for task management
#   todo:
#     type: "builtin"
#     name: "todo"
#   
#   # Fetch server for web content
#   fetch:
#     type: "builtin"
#     name: "fetch"
#   
#   # Remote MCP servers - connect via StreamableHTTP transport
#   # Optional 'headers' field can be used for authentication and custom headers
#   websearch:
#     type: "remote"
#     url: "https://api.example.com/mcp"
#   
#   weather:
#     type: "remote"
#     url: "https://weather-mcp.example.com"
#   
#   # Legacy format still supported for backward compatibility:
#   # legacy-server:
#   #   command: npx
#   #   args: ["@modelcontextprotocol/server-filesystem", "/path"]
#   #   env:
#   #     MY_VAR: "value"

# mcpServers:

# Application settings (all optional)
# model: "anthropic:claude-sonnet-4-20250514"  # Default model to use
# max-steps: 10                                # Maximum agent steps (0 for unlimited)
# debug: false                                 # Enable debug logging
# system-prompt: "/path/to/system-prompt.txt" # System prompt text file

# Model generation parameters (all optional)
# max-tokens: 4096                             # Maximum tokens in response
# temperature: 0.7                             # Randomness (0.0-1.0)
# top-p: 0.95                                  # Nucleus sampling (0.0-1.0)
# top-k: 40                                    # Top K sampling
# stop-sequences: ["Human:", "Assistant:"]     # Custom stop sequences

# API Configuration (can also use environment variables)
# provider-api-key: "your-api-key"         # API key for OpenAI, Anthropic, or Google
# provider-url: "https://api.openai.com/v1" # Base URL for OpenAI, Anthropic, or Ollama
